{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c027259-604c-428d-8e7b-aa3cb6d684e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL JDBC Example\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c22eb3-e716-4778-8764-c18d3fbdccc5",
   "metadata": {},
   "source": [
    "## read data in Sqoop HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a476e1-c011-4e44-932f-fbdcb260e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop_hdfs_path = \"hdfs://sqoop-server:9000/user/root/data/sqoop_dictory\"\n",
    "\n",
    "# read the directory as a DataFrame\n",
    "df_sqoop = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .load(sqoop_hdfs_path)\n",
    "\n",
    "df_sqoop.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa40fa-0d60-47a9-8efc-2d8aab9fb252",
   "metadata": {},
   "source": [
    "## write read data MySQL by spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17876b57-19f6-42c3-81b4-cc47d5ed67d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure the MySQL JDBC connection properties\n",
    "mysql_url = \"jdbc:mysql://mysql:3306/your_db\"\n",
    "\n",
    "mysql_properties = {\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\",\n",
    "    \"url\": mysql_url,\n",
    "    \"user\": \"your_user\",\n",
    "    \"password\": \"your_pass\",\n",
    "    \"useSSL\": \"false\",\n",
    "    \"autoReconnect\": \"true\",\n",
    "    \"zeroDateTimeBehavior\": \"convertToNull\"\n",
    "}\n",
    "\n",
    "# write the DataFrame to the MySQL database\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"dbtable\", \"your_table\") \\\n",
    "    .options(**mysql_properties) \\\n",
    "    .save()\n",
    "\n",
    "# read the MySQL table to DataFrame\n",
    "df_mysql = spark.read.jdbc(url=mysql_url, table=\"your_table\", properties=mysql_properties)\n",
    "df_mysql.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
